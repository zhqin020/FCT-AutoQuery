#!/usr/bin/env python3
"""
Federal Court Case Scraper - Main Entry Point
è”é‚¦æ³•é™¢æ¡ˆä»¶æŠ“å–å™¨ - ä¸»å…¥å£ç¨‹åº

This script demonstrates how to use the Federal Court Case Scraper
to automatically query and export case information from the Canadian
Federal Court website.

Usage:
    python main.py [case_url]

Example:
    python main.py "https://www.fct-cf.ca/en/court-files-and-decisions/IMM-12345-22"
"""

import argparse
import sys
from datetime import datetime
from pathlib import Path

# Add src to path for imports
sys.path.insert(0, str(Path(__file__).parent / "src"))

from src.lib.logging_config import setup_logging
from src.services.case_scraper_service import CaseScraperService
from src.services.export_service import ExportService

# Setup logging
setup_logging()


def main():
    """Main entry point for the Federal Court Case Scraper."""
    parser = argparse.ArgumentParser(
        description="Federal Court Case Scraper - è”é‚¦æ³•é™¢æ¡ˆä»¶è‡ªåŠ¨æŸ¥è¯¢ç³»ç»Ÿ",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹ / Usage Examples:

1. æŠ“å–å•ä¸ªæ¡ˆä»¶ / Scrape single case:
   python main.py "https://www.fct-cf.ca/en/court-files-and-decisions/IMM-12345-22"

2. æ‰¹é‡æŠ“å–å¤šä¸ªæ¡ˆä»¶ / Batch scrape multiple cases:
   python main.py --batch cases.txt

3. æŒ‡å®šè¾“å‡ºç›®å½• / Specify output directory:
   python main.py --output ./results "https://www.fct-cf.ca/en/court-files-and-decisions/IMM-12345-22"

æ³¨æ„äº‹é¡¹ / Important Notes:
- ç¨‹åºä¼šè‡ªåŠ¨éµå®ˆé€Ÿç‡é™åˆ¶ (1ç§’é—´éš”)
- æ‰€æœ‰æ“ä½œéƒ½ä¼šè®°å½•åˆ°æ—¥å¿—ä¸­
- ç¨‹åºä¼šè‡ªåŠ¨éªŒè¯URLçš„æœ‰æ•ˆæ€§
- å¦‚é‡è¿ç»­é”™è¯¯ä¼šè§¦å‘ç´§æ€¥åœæ­¢æœºåˆ¶
        """,
    )

    parser.add_argument(
        "url", nargs="?", help="è”é‚¦æ³•é™¢æ¡ˆä»¶URL / Federal Court case URL"
    )

    parser.add_argument(
        "--batch",
        type=str,
        help="åŒ…å«å¤šä¸ªURLçš„æ–‡ä»¶è·¯å¾„ / File containing multiple URLs (one per line)",
    )

    parser.add_argument(
        "--output",
        type=str,
        default="./output",
        help="è¾“å‡ºç›®å½• / Output directory (default: ./output)",
    )

    parser.add_argument(
        "--format",
        choices=["json", "csv", "both"],
        default="both",
        help="å¯¼å‡ºæ ¼å¼ / Export format (default: both)",
    )

    parser.add_argument(
        "--headless",
        action="store_true",
        default=True,
        help="æ— å¤´æ¨¡å¼è¿è¡Œæµè§ˆå™¨ / Run browser in headless mode (default: True)",
    )

    parser.add_argument(
        "--no-headless",
        action="store_false",
        dest="headless",
        help="æ˜¾ç¤ºæµè§ˆå™¨çª—å£ / Show browser window",
    )

    args = parser.parse_args()

    # Validate arguments
    if not args.url and not args.batch:
        parser.error(
            "å¿…é¡»æä¾›æ¡ˆä»¶URLæˆ–æ‰¹é‡æ–‡ä»¶ / Must provide either a case URL or batch file"
        )

    if args.url and args.batch:
        parser.error(
            "ä¸èƒ½åŒæ—¶æŒ‡å®šURLå’Œæ‰¹é‡æ–‡ä»¶ / Cannot specify both URL and batch file"
        )

    try:
        # Initialize services
        print(
            "ğŸš€ åˆå§‹åŒ–è”é‚¦æ³•é™¢æ¡ˆä»¶æŠ“å–å™¨... / Initializing Federal Court Case Scraper..."
        )
        scraper = CaseScraperService(headless=args.headless)
        exporter = ExportService(output_dir=args.output)

        cases = []

        if args.url:
            # Single case scraping
            print(f"ğŸ“„ æ­£åœ¨æŠ“å–æ¡ˆä»¶: {args.url}")
            print(f"ğŸ“„ Scraping case: {args.url}")

            case = scraper.scrape_single_case(args.url)
            cases.append(case)

            print("âœ… æ¡ˆä»¶æŠ“å–æˆåŠŸ! / Case scraped successfully!")
            print(f"   æ¡ˆä»¶ç¼–å·: {case.case_number}")
            print(f"   æ ‡é¢˜: {case.title}")
            print(f"   æ—¥æœŸ: {case.date}")
            print(f"   Case Number: {case.case_number}")
            print(f"   Title: {case.title}")
            print(f"   Date: {case.date}")

        elif args.batch:
            # Batch scraping
            batch_file = Path(args.batch)
            if not batch_file.exists():
                print(f"âŒ æ‰¹é‡æ–‡ä»¶ä¸å­˜åœ¨: {args.batch}")
                print(f"âŒ Batch file not found: {args.batch}")
                return 1

            print(f"ğŸ“‹ æ­£åœ¨è¯»å–æ‰¹é‡æ–‡ä»¶: {args.batch}")
            print(f"ğŸ“‹ Reading batch file: {args.batch}")

            with open(batch_file, "r", encoding="utf-8") as f:
                urls = [
                    line.strip()
                    for line in f
                    if line.strip() and not line.startswith("#")
                ]

            print(f"ğŸ“„ å‘ç° {len(urls)} ä¸ªURL / Found {len(urls)} URLs")

            for i, url in enumerate(urls, 1):
                try:
                    print(f"ğŸ”„ æ­£åœ¨å¤„ç† ({i}/{len(urls)}): {url}")
                    print(f"ğŸ”„ Processing ({i}/{len(urls)}): {url}")

                    case = scraper.scrape_single_case(url)
                    cases.append(case)

                    print(f"   âœ… æˆåŠŸ: {case.case_number}")
                    print(f"   âœ… Success: {case.case_number}")

                except Exception as e:
                    print(f"   âŒ å¤±è´¥: {e}")
                    print(f"   âŒ Failed: {e}")

                    # Check for emergency stop
                    if scraper.is_emergency_stop_active():
                        print(
                            "ğŸš¨ ç´§æ€¥åœæ­¢å·²æ¿€æ´»ï¼Œåœæ­¢æ‰€æœ‰æ“ä½œ / Emergency stop activated, halting all operations"
                        )
                        break

        # Export results
        if cases:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            base_filename = f"federal_court_cases_{timestamp}"

            print(
                f"\nğŸ“Š æ­£åœ¨å¯¼å‡º {len(cases)} ä¸ªæ¡ˆä»¶... / Exporting {len(cases)} cases..."
            )

            if args.format == "json":
                json_file = exporter.export_to_json(cases, f"{base_filename}.json")
                print(f"ğŸ“„ JSONæ–‡ä»¶å·²ä¿å­˜: {json_file}")
                print(f"ğŸ“„ JSON file saved: {json_file}")

            elif args.format == "csv":
                csv_file = exporter.export_to_csv(cases, f"{base_filename}.csv")
                print(f"ğŸ“„ CSVæ–‡ä»¶å·²ä¿å­˜: {csv_file}")
                print(f"ğŸ“„ CSV file saved: {csv_file}")

            else:  # both
                files = exporter.export_all_formats(cases, base_filename)
                print(f"ğŸ“„ æ–‡ä»¶å·²ä¿å­˜ / Files saved:")
                print(f"   JSON: {files['json']}")
                print(f"   CSV: {files['csv']}")

            print("\nğŸ‰ æ‰€æœ‰æ“ä½œå®Œæˆ! / All operations completed!")
            print(f"ğŸ“ è¾“å‡ºç›®å½•: {args.output}")
            print(f"ğŸ“ Output directory: {args.output}")

        else:
            print("\nâŒ æœªæˆåŠŸæŠ“å–ä»»ä½•æ¡ˆä»¶ / No cases were successfully scraped")
            return 1

    except KeyboardInterrupt:
        print("\nâ¹ï¸  ç”¨æˆ·ä¸­æ–­æ“ä½œ / Operation interrupted by user")
        return 130

    except Exception as e:
        print(f"\nâŒ å‘ç”Ÿé”™è¯¯: {e}")
        print(f"\nâŒ Error occurred: {e}")
        return 1

    finally:
        # Always cleanup
        scraper.cleanup()

    return 0


if __name__ == "__main__":
    sys.exit(main())
