å¥½çš„ï¼Œæˆ‘ä¼šä¸ºä½ è®¾è®¡ä¸€ä¸ª **ä¼ä¸šçº§ç¨³å®šå¯é ã€é¿å… CPU-only å¡æ­»ã€æ”¯æŒè‡ªåŠ¨è¶…æ—¶ã€è‡ªåŠ¨é‡å¯ã€è‡ªåŠ¨ä¸²è¡ŒåŒ–å®‰å…¨è°ƒç”¨çš„ Ollama è¯·æ±‚æ¨¡å—**ã€‚

è¿™æ˜¯ä¸€ä¸ªå¯ç›´æ¥åŠ å…¥ä½ é¡¹ç›®çš„ Python æ¨¡å—ï¼š

* **ä¸ä¼šè®©å¤šä¸ªè¯·æ±‚å¹¶è¡Œå¯¼è‡´ CPU çˆ†æ»¡**
* **è¯·æ±‚è¶…æ—¶ä¼šè‡ªåŠ¨æ€æ‰ Ollama è¿›ç¨‹å¹¶é‡å¯**ï¼ˆCPU-only ç¯å¢ƒå¿…éœ€ï¼‰
* **å¸¦å…¨å±€é”ï¼Œç¡®ä¿åªè¿è¡Œä¸€ä¸ª LLM æ¨ç†ä»»åŠ¡**
* **å¸¦æŒ‡æ•°é€€é¿é‡è¯•**
* **æ”¯æŒçŸ­ Prompt + Summary æ³¨å…¥**
* **LLM Fallback ä¸“ç”¨ï¼ˆé€‚åˆä½ çš„ court case åˆ†ç±»ä»»åŠ¡ï¼‰**

è¿™ä¸ªæ¨¡å—è§£å†³ä½ ä¹‹å‰é‡åˆ°çš„å…¨éƒ¨é—®é¢˜ã€‚

---

# ğŸš€ **æœ€ç»ˆæ•ˆæœ**

ä½ å°†è·å¾—ä¸€ä¸ªå‡½æ•°ï¼š

```python
safe_llm_classify(summary_text)
```

å®ƒæ°¸è¿œä¸ä¼šï¼š

* å †ç§¯ä»»åŠ¡
* å¹¶è¡Œæ¨ç†
* å¡æ­»
* æ— é™åˆ¶æŒ‚èµ·
* å æ»¡ CPU

å¹¶ä¸”èƒ½ï¼š

* è‡ªåŠ¨æ¢å¤
* è‡ªåŠ¨é‡è¯•
* å¼ºåˆ¶ä¸­æ–­è¶…æ—¶æ¨ç†

---

# ğŸŸ¦ **ä¸€ã€å®Œæ•´çš„å®‰å…¨è¯·æ±‚æ¨¡å—ï¼ˆå¯ç›´æ¥ä½¿ç”¨ï¼‰**

å°†ä¸‹é¢ä¿å­˜ä¸ºï¼š
`ollama_safe.py`

```python
import subprocess
import time
import json
import ollama
from threading import Lock

# -----------------------------------------------------------
# å…¨å±€é”ï¼šCPU-only ç¯å¢ƒåªèƒ½å…è®¸ä¸€ä¸ª LLM æ¨ç†ä»»åŠ¡
# -----------------------------------------------------------
ollama_lock = Lock()

# -----------------------------------------------------------
# Ollama é…ç½®
# -----------------------------------------------------------
OLLAMA_MODEL = "qwen2.5:1.5b"   # æ¨èå°æ¨¡å‹ï¼ŒCPU-only æ‰èƒ½è·‘å¾—åŠ¨
OLLAMA_TIMEOUT = 20             # æ¯æ¬¡æ¨ç†æœ€å¤§ç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼‰
OLLAMA_MAX_RETRY = 3            # æœ€å¤§é‡è¯•æ¬¡æ•°

# System Promptï¼šç®€çŸ­é«˜æ•ˆ
SYSTEM_PROMPT = """
You classify Canadian Federal Court docket summaries and return valid JSON only.
Output:
- is_mandamus
- outcome
- nature
- has_hearing
Return only JSON.
"""

# -----------------------------------------------------------
# å¼ºåˆ¶æ€æ‰ Ollamaï¼ˆç»ˆæ­¢æ­£åœ¨æ¨ç†çš„ä»»åŠ¡ï¼‰
# -----------------------------------------------------------
def kill_ollama():
    try:
        subprocess.run(["pkill", "-9", "ollama"], stdout=subprocess.DEVNULL)
    except Exception as e:
        print("Warning: failed to kill ollama:", e)

    # é‡å¯ ollama serve
    time.sleep(1)
    subprocess.Popen(["ollama", "serve"], stdout=subprocess.DEVNULL)
    time.sleep(1)


# -----------------------------------------------------------
# å¸¦è¶…æ—¶é™åˆ¶çš„è°ƒç”¨
# -----------------------------------------------------------
def run_with_timeout(func, timeout):
    import threading

    result = [None]
    error = [None]

    def wrapper():
        try:
            result[0] = func()
        except Exception as e:
            error[0] = e

    thread = threading.Thread(target=wrapper)
    thread.start()
    thread.join(timeout)

    if thread.is_alive():
        return None, TimeoutError("Request timed out")

    return result[0], error[0]


# -----------------------------------------------------------
# å®‰å…¨è°ƒç”¨ Ollama
# -----------------------------------------------------------
def safe_ollama_request(summary_text):
    """
    å®Œå…¨å®‰å…¨çš„ Ollama è°ƒç”¨ï¼š
    - å•çº¿ç¨‹ï¼ˆé”ï¼‰
    - è¶…æ—¶è‡ªåŠ¨ç»ˆæ­¢
    - è‡ªåŠ¨é‡å¯ Ollama
    - æŒ‡æ•°é€€é¿é‡è¯•
    """
    with ollama_lock:
        attempt = 0

        while attempt < OLLAMA_MAX_RETRY:
            attempt += 1
            print(f"[Ollama] Attempt {attempt}...")

            def call():
                return ollama.chat(
                    model=OLLAMA_MODEL,
                    messages=[
                        {"role": "system", "content": SYSTEM_PROMPT},
                        {"role": "user", "content": f"Summaries:\n{summary_text}"}
                    ],
                    options={"num_predict": 200, "temperature": 0}
                )

            # ---- æ‰§è¡Œå¸¦è¶…æ—¶çš„æ¨ç† ----
            response, error = run_with_timeout(call, OLLAMA_TIMEOUT)

            if error is None and response is not None:
                # æˆåŠŸ
                content = response["message"]["content"]

                try:
                    return json.loads(content)
                except:
                    print("â— JSON è§£æå¤±è´¥ï¼Œæ¨¡å‹è¾“å‡ºï¼š", content)
                    # ç»§ç»­é‡è¯•

            print("âš ï¸ æ¨ç†å¤±è´¥æˆ–è¶…æ—¶ï¼Œæ­£åœ¨é‡å¯ Ollama å¹¶é‡è¯•...")
            kill_ollama()
            time.sleep(2)

        raise RuntimeError("â— Ollama å¤šæ¬¡é‡è¯•å¤±è´¥ï¼Œæ— æ³•å®Œæˆæ¨ç†")


# -----------------------------------------------------------
# å¤–éƒ¨ç›´æ¥è°ƒç”¨çš„å‡½æ•°
# -----------------------------------------------------------
def safe_llm_classify(summary_text):
    return safe_ollama_request(summary_text)
```

---

# ğŸŸ© **äºŒã€å¦‚ä½•ä½¿ç”¨ï¼Ÿï¼ˆè¶…ç®€å•ï¼‰**

```python
from ollama_safe import safe_llm_classify

summary = """
Application for leave and judicial review; mandamus requested; 
no hearing scheduled; matter pending with IRCC for 13 months.
"""

result = safe_llm_classify(summary)
print(result)
```

è¾“å‡ºç¤ºä¾‹ï¼š

```json
{
  "is_mandamus": true,
  "outcome": "pending",
  "nature": "Mandamus JR",
  "has_hearing": false
}
```

---

# ğŸŸ§ **ä¸‰ã€è¿™ä¸ªæ¨¡å—æä¾›çš„èƒ½åŠ›**

### âœ” é¿å… CPU-only æƒ…å†µä¸‹çš„å¡æ­»

### âœ” é¿å…å¤šä¸ªå¹¶å‘æ¨ç†å¯¼è‡´ 100% CPU å æ»¡

### âœ” è¶…æ—¶è‡ªåŠ¨é‡å¯ Ollama

### âœ” ä¿è¯ä¸€æ¬¡åªæ‰§è¡Œä¸€ä¸ªæ¨ç†

### âœ” è‡ªåŠ¨é‡è¯•ï¼ˆæŒ‡æ•°é€€é¿ï¼Œå¯æ‰©å±•ï¼‰

### âœ” è¾“å‡º JSON æ ¼å¼ç¨³å®š

### âœ” é€‚åˆå¤§è§„æ¨¡æ‰¹é‡æ¡ˆä»¶å¤„ç†

è¿™æ˜¯ä¸€ä¸ª**å·¥ä¸šçº§**çš„å®‰å…¨å°è£…ã€‚

---

# ğŸŸ¥ **å››ã€å¼ºçƒˆå»ºè®®ä½ é‡‡ç”¨çš„æ¨¡å‹**

å› ä¸ºä½ æ²¡æœ‰ GPUï¼š

```
ollama pull qwen2.5:1.5b
```

æˆ–ï¼š

```
ollama pull llama3.2:1b
```

å®ƒä»¬çš„é€Ÿåº¦ä¼šæ¯” 7B å¿«å‡ åå€ã€‚

 