import pandas as pd
import numpy as np

# Suppress future warning for downcasting
pd.set_option('future.no_silent_downcasting', True)
from sqlalchemy import create_engine, text
import re
import sys
import json
import argparse
from datetime import datetime
import os
import matplotlib
matplotlib.use('Agg')  # Use non-interactive backend for headless environments

# é…ç½®è¾“å‡ºç›®å½•
OUTPUT_DIR = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))), 'output')
os.makedirs(OUTPUT_DIR, exist_ok=True)

import matplotlib.pyplot as plt
import seaborn as sns
from sqlalchemy.exc import OperationalError as SAOperationalError
from lib.config import Config
import matplotlib.font_manager as fm
from matplotlib.font_manager import FontProperties

# è®¾ç½® matplotlib å’Œ seaborn æ ·å¼ä»¥è·å¾—æ›´å¥½çš„å›¾è¡¨è§†è§‰æ•ˆæœ
sns.set_style("whitegrid")


# Global variable to store CJK font path
_cjk_font_path = None

def get_cjk_font_prop():
    """è¿”å›ç¬¬ä¸€ä¸ªå¯ç”¨çš„ç³»ç»Ÿ CJK å­—ä½“çš„ FontPropertiesï¼Œæˆ– Noneã€‚

    è¯¥å‡½æ•°ä¼šå°è¯•å‡ ä¸ªå¸¸è§çš„ç³»ç»Ÿå­—ä½“è·¯å¾„ï¼ˆåŒ…æ‹¬ Noto CJK å’Œ WQYï¼‰ï¼Œå¹¶æ„é€ 
    ä¸€ä¸ª FontProperties æŒ‡å‘è¯¥å­—ä½“æ–‡ä»¶ï¼Œä¾¿äºåœ¨ç»˜å›¾æ—¶æ˜¾å¼ä¼ å…¥ä»¥ä¿è¯ä¸­æ–‡æ˜¾ç¤ºã€‚
    """
    global _cjk_font_path
    candidate_paths = [
        '/usr/share/fonts/opentype/noto/NotoSansCJK-Regular.ttc',
        '/usr/share/fonts/opentype/noto/NotoSerifCJK-Regular.ttc',
        '/usr/share/fonts/truetype/wqy/wqy-zenhei.ttc',
        '/usr/share/fonts/truetype/wqy/wqy-zenhei.ttf',
    ]
    for p in candidate_paths:
        if os.path.exists(p):
            try:
                _cjk_font_path = p
                return FontProperties(fname=p)
            except Exception:
                continue
    # fallback: try to find any system font whose filename hints at CJK
    for p in fm.findSystemFonts(fontpaths=None, fontext='ttf') + fm.findSystemFonts(fontpaths=None, fontext='otf'):
        lp = p.lower()
        if any(k in lp for k in ('noto', 'wqy', 'yahei', 'msyh', 'simhei', 'ukai', 'kaiu')):
            try:
                _cjk_font_path = p
                return FontProperties(fname=p)
            except Exception:
                continue
    return None


# å°è¯•è·å–ä¸€ä¸ª FontPropertiesï¼Œç”¨äºåœ¨ç»˜å›¾æ—¶ä¿è¯ä¸­æ–‡æ–‡æœ¬ä½¿ç”¨å¯ç”¨å­—ä½“
_cjk_prop = get_cjk_font_prop()
if not _cjk_prop:
    print("æ³¨æ„ï¼šæœªæ£€æµ‹åˆ°æ¨èçš„ä¸­æ–‡å­—ä½“ï¼Œå›¾è¡¨ä¸­æ–‡å¯èƒ½æ— æ³•æ­£ç¡®æ˜¾ç¤ºã€‚")
    print("å»ºè®®åœ¨ç³»ç»Ÿä¸Šå®‰è£…å­—ä½“ï¼Œä¾‹å¦‚ (Debian/Ubuntu): sudo apt-get install fonts-noto-cjk fonts-wqy-zenhei")

plt.rcParams['axes.unicode_minus'] = False  # è§£å†³è´Ÿå·æ˜¾ç¤ºé—®é¢˜
if _cjk_prop:
    try:
        fam = _cjk_prop.get_name()
        if fam:
            plt.rcParams['font.sans-serif'] = [fam]
            plt.rcParams['font.family'] = 'sans-serif'
    except Exception:
        pass
    # If we have a font file path, monkeypatch font_manager.findfont to always return it
    try:
        if '_cjk_font_path' in globals() and _cjk_font_path:
            # ensure font file is registered with matplotlib's font manager
            try:
                fm.fontManager.addfont(_cjk_font_path)
            except Exception:
                pass
            # after registering, try to set the sans-serif family to the font's family name
            try:
                fam2 = FontProperties(fname=_cjk_font_path).get_name()
                if fam2:
                    plt.rcParams['font.sans-serif'] = [fam2]
                    plt.rcParams['font.family'] = 'sans-serif'
            except Exception:
                pass
            # as a last resort, force findfont to return the font file path
            def _forced_findfont(*args, **kwargs):
                return _cjk_font_path
            fm.findfont = _forced_findfont
    except Exception:
        pass

# =================é…ç½®åŒºåŸŸ=================
# ä¼˜å…ˆä½¿ç”¨ç¯å¢ƒå˜é‡ DB_CONNECTION_STRï¼Œå…¶æ¬¡ä» Config.get_db_config() ä¸­è¯»å–å¹¶æ„å»º DSN
# Config.get_db_config() è¿”å›: { 'host','port','database','user','password' }
db_cfg = Config.get_db_config() or {}
env_dsn = os.getenv('DB_CONNECTION_STR')
if env_dsn:
    DB_CONNECTION_STR = env_dsn
else:
    DB_CONNECTION_STR = f"postgresql://{db_cfg.get('user')}:{db_cfg.get('password')}@{db_cfg.get('host')}:{db_cfg.get('port')}/{db_cfg.get('database')}"
# è‹¥ä½¿ç”¨ç¯å¢ƒå˜é‡ï¼Œåœ¨è¿è¡Œå‰è®¾ç½®: export DB_CONNECTION_STR='postgresql://user:pass@host:5432/db'
# æ‚¨çš„æ¡ˆå­æäº¤ DOJ Memo çš„æ—¥æœŸ (ç”¨äºè®¡ç®—æ‚¨çš„é™é»˜æœŸ)
MY_CASE_MEMO_DATE = '2025-07-30' # ç¤ºä¾‹æ—¥æœŸï¼Œè¯·æ›¿æ¢ä¸ºå®é™…æ—¥æœŸ
# =========================================

# --- æ•°æ®åº“äº¤äº’éƒ¨åˆ† ---

def get_mandamus_data_for_analysis(year=2025):
    """ä»æ•°æ®åº“æ‹‰å–æŒ‡å®šå¹´ä»½çš„ Mandamus æ¡ˆä»¶æ•°æ®"""
    engine = create_engine(DB_CONNECTION_STR)
    
    # æ‹‰å– case_analysis çš„æ ¸å¿ƒæ•°æ®ï¼Œä»…é™æŒ‡å®šå¹´ä»½ï¼Œå¹¶ç¡®ä¿æ—¥æœŸæ ¼å¼æ­£ç¡®
    year_suffix = f"-{year % 100:02d}"
    query = f"""
    SELECT 
        case_id AS case_number,
        filing_date,
        case_status,
        visa_office,
        time_to_close,
        outcome_date,
        memo_response_time,
        reply_memo_date,
        reply_to_outcome_time
    FROM case_analysis 
    WHERE case_type = 'Mandamus' 
    AND (case_id LIKE '%{year_suffix}' OR case_number LIKE '%{year_suffix}')
    ORDER BY filing_date ASC;
    """
    
    print(f"æ­£åœ¨æå– {year} å¹´ Mandamus æ¡ˆä»¶æ ¸å¿ƒæ•°æ®...")
    try:
        with engine.connect() as connect:
            df = pd.read_sql(text(query), connect)
    except SAOperationalError as e:
        print("æ•°æ®åº“è¿æ¥å¤±è´¥ï¼š", str(e))
        print("è¯·æ£€æŸ¥é…ç½®æˆ–ç¯å¢ƒå˜é‡ DB_CONNECTION_STRï¼Œæˆ–ç¡®ä¿æ•°æ®åº“å‡­æ®åœ¨ Config ä¸­æ­£ç¡®è®¾ç½®ï¼ˆget_db_configï¼‰ã€‚")
        return pd.DataFrame()
    except Exception as e:
        print("è¯»å–æ•°æ®åº“æ—¶å‘ç”Ÿé”™è¯¯ï¼š", str(e))
        return pd.DataFrame()
    
    df['filing_date'] = pd.to_datetime(df['filing_date'], errors='coerce')
    df['outcome_date'] = pd.to_datetime(df['outcome_date'], errors='coerce')
    
    print(f"æå–å®Œæˆ: {len(df)} æ¡ {year} å¹´è®°å½•")
    return df


def export_cases_to_json(year=2025):
    """æå– Granted å’Œ Dismissed æ¡ˆä»¶çš„åŸå§‹ä¿¡æ¯å’Œåˆ†æç»“æœï¼Œå¹¶ä¿å­˜ä¸º JSONã€‚"""
    engine = create_engine(DB_CONNECTION_STR)
    
    for status in ['Granted', 'Dismissed']:
        filename_base = f"{status.lower()}_cases_{year}.json"
        filename = os.path.join(OUTPUT_DIR, filename_base)
        print(f"\næ­£åœ¨å¯¼å‡º {status} æ¡ˆä»¶åˆ° {filename}...")
        
        # 1. ä» case_analysis è·å–è¯¥çŠ¶æ€çš„ Mandamus æ¡ˆä»¶
        year_suffix = f"-{year % 100:02d}"
        analysis_query = f"""
        SELECT * FROM case_analysis 
        WHERE case_type = 'Mandamus' 
        AND case_status = '{status}'
        AND (case_id LIKE '%{year_suffix}' OR case_number LIKE '%{year_suffix}')
        """
        with engine.connect() as connect:
            analysis_df = pd.read_sql(text(analysis_query), connect)
        
        if analysis_df.empty:
            print(f"   (æœªå‘ç° {year} å¹´ {status} çŠ¶æ€ of Mandamus æ¡ˆä»¶æ•°æ®)")
            continue
            
        case_ids = analysis_df['case_id'].tolist()
        
        # 2. è·å– cases è¡¨çš„åŸå§‹åŸºæœ¬ä¿¡æ¯
        cases_info_list = []
        batch_size = 500
        for i in range(0, len(case_ids), batch_size):
            batch = case_ids[i:i + batch_size]
            batch_str = ",".join([f"'{c}'" for c in batch])
            c_query = f"SELECT * FROM cases WHERE case_number IN ({batch_str})"
            with engine.connect() as connect:
                batch_df = pd.read_sql(text(c_query), connect)
            cases_info_list.append(batch_df)
        
        cases_df = pd.concat(cases_info_list) if cases_info_list else pd.DataFrame()
        
        # 3. è·å–æ‰€æœ‰ç›¸å…³çš„ docket_entries
        docket_list = []
        for i in range(0, len(case_ids), batch_size):
            batch = case_ids[i:i + batch_size]
            batch_str = ",".join([f"'{c}'" for c in batch])
            d_query = f"SELECT * FROM docket_entries WHERE case_number IN ({batch_str}) ORDER BY date_filed ASC"
            with engine.connect() as connect:
                batch_df = pd.read_sql(text(d_query), connect)
            docket_list.append(batch_df)
            
        docket_df = pd.concat(docket_list) if docket_list else pd.DataFrame()
        
        # 4. ç»„è£…æ•°æ®æ„å»º JSON æ ¼å¼
        json_results = []
        
        # è¾…åŠ©æ—¥æœŸå¤„ç†å‡½æ•°
        def date_handler(obj):
            if hasattr(obj, 'isoformat'):
                return obj.isoformat()
            return obj

        for _, analysis_row in analysis_df.iterrows():
            c_num = analysis_row['case_id']
            
            # è·å–åŸºæœ¬ä¿¡æ¯å­—å…¸
            c_info = cases_df[cases_df['case_number'] == c_num].to_dict('records')
            c_info_dict = c_info[0] if c_info else {}
            
            # è·å–è¯¥æ¡ˆçš„æ‰€æœ‰ docket entries
            entries = docket_df[docket_df['case_number'] == c_num].to_dict('records')
            
            # åˆå¹¶ä¸ºä¸€ä¸ªå¯¹è±¡
            json_results.append({
                "case_number": c_num,
                "analysis_result": {k: date_handler(v) for k, v in analysis_row.to_dict().items()},
                "raw_case_info": {k: date_handler(v) for k, v in c_info_dict.items()},
                "docket_entries": [{k: date_handler(v) for k, v in e.items()} for e in entries]
            })
            
        # å†™å…¥æ–‡ä»¶
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(json_results, f, ensure_ascii=False, indent=2)
            print(f"âœ… å·²æˆåŠŸç”Ÿæˆ {filename} (å« {len(json_results)} ä¸ªæ¡ˆä»¶)")
        except Exception as e:
            print(f"âŒ å†™å…¥ {filename} å¤±è´¥: {e}")

# --- åˆ†æå’Œå¯è§†åŒ–éƒ¨åˆ† ---

def plot_workload_trends(df_monthly):
    """ç»˜åˆ¶æ¯æœˆæ³¨å†Œé‡ã€ç»“æ¡ˆé‡å’Œå‡€ç§¯å‹å˜åŒ–è¶‹åŠ¿å›¾"""

    fig, ax1 = plt.subplots(figsize=(12, 6))

    # ç»˜åˆ¶æ³¨å†Œé‡ (Filing Count)
    ax1.plot(df_monthly.index, df_monthly['filing_count'], marker='o', linestyle='-', color='tab:blue', label='æ¡ˆä»¶æ³¨å†Œé‡')
    if _cjk_prop:
        ax1.set_xlabel('æœˆä»½', fontproperties=_cjk_prop)
        ax1.set_ylabel('æ¡ˆä»¶æ•°é‡', color='tab:blue', fontproperties=_cjk_prop)
    else:
        ax1.set_xlabel('æœˆä»½')
        ax1.set_ylabel('æ¡ˆä»¶æ•°é‡', color='tab:blue')
    ax1.tick_params(axis='y', labelcolor='tab:blue')

    # ç¬¬äºŒä¸ªYè½´ç»˜åˆ¶å‡€ç§¯å‹å˜åŒ– (Net Change)
    ax2 = ax1.twinx() 
    ax2.bar(df_monthly.index, df_monthly['net_change'], width=20, alpha=0.6, color=np.where(df_monthly['net_change'] >= 0, 'tab:red', 'tab:green'), label='å‡€ç§¯å‹å˜åŒ–')
    if _cjk_prop:
        ax2.set_ylabel('å‡€ç§¯å‹å˜åŒ– (æ³¨å†Œ - ç»“æ¡ˆ)', color='tab:red', fontproperties=_cjk_prop)
    else:
        ax2.set_ylabel('å‡€ç§¯å‹å˜åŒ– (æ³¨å†Œ - ç»“æ¡ˆ)', color='tab:red')
    ax2.tick_params(axis='y', labelcolor='tab:red')

    fig.autofmt_xdate(rotation=45)
    if _cjk_prop:
        ax1.set_title('2025 å¹´ Mandamus æ¡ˆä»¶æ¯æœˆè´Ÿè·åŠç§¯å‹è¶‹åŠ¿', fontproperties=_cjk_prop)
        leg = ax1.legend(loc='upper left', prop=_cjk_prop)
    else:
        plt.title('2025 å¹´ Mandamus æ¡ˆä»¶æ¯æœˆè´Ÿè·åŠç§¯å‹è¶‹åŠ¿')
        leg = ax1.legend(loc='upper left')
    # ensure x tick labels use CJK font if available
    if _cjk_prop:
        for lbl in ax1.get_xticklabels():
            lbl.set_fontproperties(_cjk_prop)
    save_path = os.path.join(OUTPUT_DIR, 'mandamus_workload_trends.png')
    plt.savefig(save_path)
    print(f"ğŸ“ˆ å·²ä¿å­˜è´Ÿè½½è¶‹åŠ¿å›¾è‡³: {save_path}")
    plt.close()


def plot_outcome_trends(df_monthly):
    """ç»˜åˆ¶æ¯æœˆç»“æ¡ˆæ–¹å¼è¶‹åŠ¿å›¾"""

    # å †å å›¾æ•°æ®å‡†å¤‡ï¼šåªçœ‹å·²ç»“æ¡ˆçš„éƒ¨åˆ†
    df_outcome_plot = df_monthly[['settled_count', 'dismissed_count', 'granted_count']].fillna(0)

    # å°†å…¶ä»–æ–¹å¼ç»“æ¡ˆåˆå¹¶ä¸º "Other/Dismissed"
    df_outcome_plot['Other/Dismissed'] = df_outcome_plot['dismissed_count'] # å‡è®¾è´¥è¯‰å æ¯”æœ€å¤š
    df_outcome_plot['Settled'] = df_outcome_plot['settled_count']
    df_outcome_plot['Granted'] = df_outcome_plot['granted_count']

    fig, ax = plt.subplots(figsize=(12, 6))
    df_outcome_plot[['Settled', 'Granted', 'Other/Dismissed']].plot(kind='bar', stacked=True, ax=ax)

    if _cjk_prop:
        ax.set_title('2025 å¹´ Mandamus æ¡ˆä»¶æ¯æœˆç»“æ¡ˆæ–¹å¼åˆ†å¸ƒ', fontproperties=_cjk_prop)
        ax.set_xlabel('æœˆä»½', fontproperties=_cjk_prop)
        ax.set_ylabel('ç»“æ¡ˆæ•°é‡', fontproperties=_cjk_prop)
        leg = ax.legend(title='ç»“æ¡ˆæ–¹å¼', prop=_cjk_prop)
        for lbl in ax.get_xticklabels():
            lbl.set_fontproperties(_cjk_prop)
        if leg:
            for text in leg.get_texts():
                text.set_fontproperties(_cjk_prop)
    else:
        ax.set_title('2025 å¹´ Mandamus æ¡ˆä»¶æ¯æœˆç»“æ¡ˆæ–¹å¼åˆ†å¸ƒ')
        ax.set_xlabel('æœˆä»½')
        ax.set_ylabel('ç»“æ¡ˆæ•°é‡')
        plt.legend(title='ç»“æ¡ˆæ–¹å¼')
    fig.autofmt_xdate(rotation=45)
    save_path = os.path.join(OUTPUT_DIR, 'mandamus_outcome_trends.png')
    plt.savefig(save_path)
    print(f"ğŸ“ˆ å·²ä¿å­˜ç»“æ¡ˆæ–¹å¼è¶‹åŠ¿å›¾è‡³: {save_path}")
    plt.close()


def plot_timeline_trends(df_monthly):
    """ç»˜åˆ¶æ¯æœˆç»“æ¡ˆè€—æ—¶è¶‹åŠ¿å›¾"""

    fig, ax = plt.subplots(figsize=(12, 6))

    # ç»˜åˆ¶å¹³å‡ç»“æ¡ˆè€—æ—¶ (ä¸­ä½æ•°)
    ax.plot(df_monthly.index, df_monthly['median_time_to_close'], marker='s', linestyle='--', color='purple', label='ä¸­ä½æ•°æ€»è€—æ—¶')

    if _cjk_prop:
        ax.set_title('2025 å¹´ Mandamus æ¡ˆä»¶ä¸­ä½æ•°ç»“æ¡ˆè€—æ—¶è¶‹åŠ¿', fontproperties=_cjk_prop)
        ax.set_xlabel('æœˆä»½', fontproperties=_cjk_prop)
        ax.set_ylabel('è€—æ—¶ (å¤©æ•°)', fontproperties=_cjk_prop)
    else:
        ax.set_title('2025 å¹´ Mandamus æ¡ˆä»¶ä¸­ä½æ•°ç»“æ¡ˆè€—æ—¶è¶‹åŠ¿')
        ax.set_xlabel('æœˆä»½')
        ax.set_ylabel('è€—æ—¶ (å¤©æ•°)')

    fig.autofmt_xdate(rotation=45)
    plt.legend()
    save_path = os.path.join(OUTPUT_DIR, 'mandamus_timeline_trends.png')
    plt.savefig(save_path)
    print(f"ğŸ“ˆ å·²ä¿å­˜ç»“æ¡ˆè€—æ—¶è¶‹åŠ¿å›¾è‡³: {save_path}")
    plt.close()


def plot_memo_response_trends(df_monthly):
    """ç»˜åˆ¶æ¯æœˆ DOJ Memo å“åº”æ—¶é—´è¶‹åŠ¿å›¾"""

    if 'median_memo_response_time' not in df_monthly.columns:
        print("æ²¡æœ‰ Memo å“åº”æ—¶é—´æ•°æ®ï¼Œè·³è¿‡å›¾è¡¨ç»˜åˆ¶ã€‚")
        return

    fig, ax = plt.subplots(figsize=(12, 6))

    # ç»˜åˆ¶å¹³å‡ Memo å“åº”æ—¶é—´ (ä¸­ä½æ•°)
    ax.plot(df_monthly.index, df_monthly['median_memo_response_time'], marker='o', linestyle='-', color='orange', label='ä¸­ä½æ•° DOJ Memo å“åº”æ—¶é—´')

    if _cjk_prop:
        ax.set_title('2025 å¹´ Mandamus æ¡ˆä»¶ DOJ Memo å“åº”æ—¶é—´è¶‹åŠ¿', fontproperties=_cjk_prop)
        ax.set_xlabel('æœˆä»½', fontproperties=_cjk_prop)
        ax.set_ylabel('å“åº”æ—¶é—´ (å¤©æ•°)', fontproperties=_cjk_prop)
    else:
        ax.set_title('2025 å¹´ Mandamus æ¡ˆä»¶ DOJ Memo å“åº”æ—¶é—´è¶‹åŠ¿')
        ax.set_xlabel('æœˆä»½')
        ax.set_ylabel('å“åº”æ—¶é—´ (å¤©æ•°)')

    fig.autofmt_xdate(rotation=45)
    plt.legend()
    save_path = os.path.join(OUTPUT_DIR, 'mandamus_memo_response_trends.png')
    plt.savefig(save_path)
    print(f"ğŸ“ˆ å·²ä¿å­˜ Memo å“åº”è¶‹åŠ¿å›¾è‡³: {save_path}")
    plt.close()


def plot_memo_reply_to_outcome_trends(df):
    """æŒ‰æœˆç»Ÿè®¡ï¼šä» memo å›å¤åˆ°ç»“æ¡ˆçš„æ—¶é—´ï¼ˆå¤©ï¼‰ï¼ŒæŒ‰ç»“æ¡ˆç±»å‹åˆ†ç³»åˆ—ç»˜å›¾ã€‚

    è®¡ç®—æ–¹æ³•:
    - ä¼˜å…ˆä½¿ç”¨ reply_memo_dateï¼ˆå®é™…çš„ DOJ Memo å›å¤æ—¥æœŸï¼‰
    - å¦‚æœ reply_memo_date ä¸ºç©ºï¼Œåˆ™ä½¿ç”¨ filing_date + memo_response_time ä½œä¸ºå¤‡é€‰
    - reply_to_outcome_days = (outcome_date or å½“å‰æ—¥æœŸ) - reply_memo_date çš„å¤©æ•°
    æŒ‰ outcome_date çš„æœˆæœ«é¢‘ç‡åˆ†ç»„ï¼Œå¹¶å¯¹æ¯ä¸ª case_status è®¡ç®—æœ€å¤§ã€æœ€å°ã€å¹³å‡ã€ä¸­ä½æ•°ã€‚
    åŒæ—¶æ˜¾ç¤º IMM-11243-25 æ¡ˆä¾‹ä» memo å›å¤åˆ°å½“å‰çš„æ—¶é—´ä½œä¸ºå‚è€ƒçº¿ã€‚
    """
    df = df.copy()
    # å¿…è¦å­—æ®µ - åŒ…å« reply_memo_date
    required_fields = {'filing_date', 'outcome_date', 'case_status', 'case_number', 'reply_memo_date'}
    optional_fields = {'memo_response_time'}  # å¤‡ç”¨å­—æ®µ
    
    if not required_fields.issubset(df.columns):
        print(f"ç¼ºå°‘å¿…è¦å­—æ®µï¼Œè·³è¿‡ reply_memo->outcome ç»Ÿè®¡ã€‚éœ€è¦ï¼š{required_fields - set(df.columns)}")
        return

    # æå–ç‰¹å®šæ¡ˆä¾‹ IMM-11243-25 çš„ä¿¡æ¯ä½œä¸ºå‚è€ƒ
    reference_days = None
    reference_start_date = None
    
    target_case = df[df['case_number'] == 'IMM-11243-25']
    if not target_case.empty:
        case_row = target_case.iloc[0]
        
        # ä¼˜å…ˆä½¿ç”¨ reply_memo_dateï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨æ‚¨æŒ‡å®šçš„æ—¥æœŸ
        if pd.notna(case_row['reply_memo_date']):
            reference_start_date = pd.to_datetime(case_row['reply_memo_date'])
        else:
            # ä½¿ç”¨æ‚¨æŒ‡å®šçš„æ—¥æœŸ 2025-07-30
            reference_start_date = pd.to_datetime('2025-07-30')
        
        if reference_start_date is not None:
            # å¯¹äºæœªç»“æ¡ˆæ¡ˆä¾‹ï¼Œä½¿ç”¨å½“å‰æ—¥æœŸï¼›å¯¹äºå·²ç»“æ¡ˆæ¡ˆä¾‹ï¼Œä½¿ç”¨outcome_date
            if pd.notna(case_row['outcome_date']):
                end_date = pd.to_datetime(case_row['outcome_date'])
                period_desc = f"è‡³ç»“æ¡ˆæ—¥æœŸ {end_date.date()}"
            else:
                end_date = pd.Timestamp.now()
                period_desc = f"è‡³ä»Šå¤© {end_date.date()}"
            
            reference_days = (end_date - reference_start_date).days
            print(f"å‚è€ƒæ¡ˆä¾‹ IMM-11243-25: memoå›å¤æ—¥æœŸ={reference_start_date.date()}, {period_desc}, å¤©æ•°={reference_days:.0f}å¤©")

    # å¤„ç†æ‰€æœ‰æ¡ˆä¾‹çš„ reply_memo_date
    # è½¬æ¢æ—¥æœŸå­—æ®µ
    df['filing_date'] = pd.to_datetime(df['filing_date'], errors='coerce')
    df['outcome_date'] = pd.to_datetime(df['outcome_date'], errors='coerce')
    df['reply_memo_date'] = pd.to_datetime(df['reply_memo_date'], errors='coerce')
    
    # è®¡ç®— reply_memo_dateï¼šä¼˜å…ˆä½¿ç”¨å®é™…å€¼ï¼Œå…¶æ¬¡ä½¿ç”¨ä¼°ç®—å€¼
    df['calculated_reply_date'] = df['reply_memo_date']  # ä¼˜å…ˆä½¿ç”¨å®é™… reply_memo_date
    
    # å¯¹äºæ²¡æœ‰ reply_memo_date ä½†æœ‰ memo_response_time çš„æ¡ˆä¾‹ï¼Œä½¿ç”¨å¤‡é€‰è®¡ç®—
    mask_need_calc = df['calculated_reply_date'].isna() & df['memo_response_time'].notna() & df['filing_date'].notna()
    if mask_need_calc.any():
        df.loc[mask_need_calc, 'calculated_reply_date'] = df.loc[mask_need_calc, 'filing_date'] + pd.to_timedelta(df.loc[mask_need_calc, 'memo_response_time'], unit='D')
        print(f"ä¸º {mask_need_calc.sum()} ä¸ªæ¡ˆä¾‹ä½¿ç”¨ä¼°ç®—çš„ memo å›å¤æ—¥æœŸ")

    # è®¡ç®— reply_to_outcome_days
    df['reply_to_outcome_days'] = None
    
    # å¯¹äºå·²ç»“æ¡ˆæ¡ˆä¾‹
    resolved_mask = df['case_status'].isin(['Discontinued', 'Granted', 'Dismissed'])
    resolved_with_dates = resolved_mask & df['outcome_date'].notna() & df['calculated_reply_date'].notna()
    
    if resolved_with_dates.any():
        df.loc[resolved_with_dates, 'reply_to_outcome_days'] = (
            df.loc[resolved_with_dates, 'outcome_date'] - df.loc[resolved_with_dates, 'calculated_reply_date']
        ).dt.days

    # å¯¹äºæœªç»“æ¡ˆæ¡ˆä¾‹ï¼ˆæœ‰ reply_memo_date ä½†æ²¡æœ‰ outcome_dateï¼‰ï¼Œè®¡ç®—åˆ°å½“å‰çš„æ—¶é—´
    unresolved_mask = ~resolved_mask & df['calculated_reply_date'].notna()
    if unresolved_mask.any():
        current_date = pd.Timestamp.now()
        df.loc[unresolved_mask, 'reply_to_outcome_days'] = (
            current_date - df.loc[unresolved_mask, 'calculated_reply_date']
        ).dt.days

    # åªç»Ÿè®¡æœ‰æ•ˆçš„æ•°æ®
    df_valid = df[df['reply_to_outcome_days'].notna() & (df['reply_to_outcome_days'] >= 0)].copy()
    
    if df_valid.empty:
        print("æ²¡æœ‰æœ‰æ•ˆçš„ reply_memo åˆ° outcome æ—¶é—´æ•°æ®ï¼Œè·³è¿‡ç»˜å›¾ã€‚")
        return

    # å¯¹äºæœˆåº¦è¶‹åŠ¿ï¼Œæˆ‘ä»¬åªçœ‹å·²ç»“æ¡ˆæ¡ˆä¾‹ï¼ˆå› ä¸º outcome_date æ˜¯åˆ†ç»„ä¾æ®ï¼‰
    df_resolved = df_valid[df_valid['case_status'].isin(['Discontinued', 'Granted', 'Dismissed'])].copy()
    if df_resolved.empty:
        print("æ²¡æœ‰å·²ç»“æ¡ˆçš„æœ‰æ•ˆæ•°æ®ç”¨äºæœˆåº¦è¶‹åŠ¿ï¼Œè·³è¿‡ç»˜å›¾ã€‚")
        return

    # æŒ‰ outcome_date æœˆæœ« å’Œ case_status åˆ†ç»„ï¼Œè®¡ç®—å¤šä¸ªç»Ÿè®¡æŒ‡æ ‡
    grouped = df_resolved.groupby([pd.Grouper(key='outcome_date', freq='ME'), 'case_status'])['reply_to_outcome_days'].agg(['mean', 'median'])
    if grouped.empty:
        print("åˆ†ç»„åæ— æ•°æ®ï¼Œè·³è¿‡ç»˜å›¾ã€‚")
        return

    # åˆ›å»º 2 ä¸ªå­å›¾ï¼Œåˆ†åˆ«æ˜¾ç¤ºå¹³å‡å€¼ã€ä¸­ä½æ•°
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))
    
    metrics = [('mean', 'å¹³å‡å€¼'), ('median', 'ä¸­ä½æ•°')]
    axes = [ax1, ax2]
    
    for (metric, metric_name), ax in zip(metrics, axes):
        # é‡ç½®ç´¢å¼•ä»¥ä¾¿äºç»˜å›¾
        pivot_data = grouped[metric].unstack(level=-1)
        
        # ä¸ºæ¯ä¸ªç»“æ¡ˆç±»å‹ç»˜åˆ¶çº¿æ¡
        for col in pivot_data.columns:
            ax.plot(pivot_data.index, pivot_data[col], marker='o', linestyle='-', label=str(col))
        
        # æ·»åŠ å‚è€ƒæ¡ˆä¾‹çš„æ°´å¹³çº¿
        if reference_days is not None:
            ax.axhline(y=reference_days, color='red', linestyle='--', linewidth=2, 
                      label=f'IMM-11243-25 ({reference_days:.0f}å¤©)')
            # é‡æ–°ç»˜åˆ¶å›¾ä¾‹ä»¥åŒ…å«å‚è€ƒçº¿
            if _cjk_prop:
                leg = ax.legend(prop=_cjk_prop)
            else:
                ax.legend()
        
        # è®¾ç½®æ ‡é¢˜å’Œæ ‡ç­¾
        if _cjk_prop:
            ax.set_title(f'Memoå›å¤åˆ°ç»“æ¡ˆæ—¶é—´ - {metric_name}ï¼ˆå¤©ï¼‰', fontproperties=_cjk_prop)
            ax.set_xlabel('ç»“æ¡ˆæœˆä»½', fontproperties=_cjk_prop)
            ax.set_ylabel('å¤©æ•°', fontproperties=_cjk_prop)
            if not _cjk_prop:
                for lbl in ax.get_xticklabels():
                    lbl.set_fontproperties(_cjk_prop)
        else:
            ax.set_title(f'Memo Reply to Outcome Time - {metric_name} (days)')
            ax.set_xlabel('Outcome Month')
            ax.set_ylabel('Days')
        
        ax.tick_params(axis='x', rotation=45)

    fig.suptitle('æŒ‰ç»“æ¡ˆç±»å‹ç»Ÿè®¡ï¼šMemoå›å¤åˆ°ç»“æ¡ˆæ—¶é—´åˆ†æï¼ˆå«IMM-11243-25å‚è€ƒçº¿ï¼‰', fontsize=16, fontproperties=_cjk_prop if _cjk_prop else None)
    fig.tight_layout()
    save_path = os.path.join(OUTPUT_DIR, 'mandamus_memo_to_outcome_trends.png')
    plt.savefig(save_path)
    print(f"ğŸ“ˆ å·²ä¿å­˜ Memo åˆ°ç»“æ¡ˆæ—¶é—´ç»Ÿè®¡å›¾è‡³: {save_path}")
    plt.close()

    # === æ‰“å°æ‘˜è¦ç»Ÿè®¡å†…å®¹ ===
    print("\n" + "="*60)
    print("ã€Memoå›å¤åˆ°ç»“æ¡ˆæ—¶é—´åˆ†ææ‘˜è¦ (æœ€è¿‘ 6 ä¸ªæœˆ)ã€‘")
    print("="*60)
    
    # 1. æ¯æœˆæ€»ä½“ç»“æ¡ˆç»Ÿè®¡ (DataFrame é£æ ¼)
    summary_overall = df_resolved.groupby(pd.Grouper(key='outcome_date', freq='ME'))['reply_to_outcome_days'].agg(['count', 'mean']).rename(columns={'count': 'resolved_count', 'mean': 'avg_days'})
    print("\n--- æ¯æœˆæ€»ä½“ç»“æ¡ˆç»Ÿè®¡ (æœ€è¿‘ 6 ä¸ªæœˆ) ---")
    if not summary_overall.empty:
        # æ ¼å¼åŒ–ç´¢å¼•ä¸º YYYY-MM å­—ç¬¦ä¸²ä»¥è·å¾—æ›´å¥½çš„æ‰“å°æ•ˆæœ
        summary_overall.index = summary_overall.index.strftime('%Y-%m')
        # å¼ºåˆ¶å°†æ•°å€¼åˆ—è½¬æ¢ä¸º float å¹¶å››èˆäº”å…¥åˆ°1ä½å°æ•°ï¼Œç¡®ä¿æ‰“å°æ•ˆæœ
        summary_overall['avg_days'] = summary_overall['avg_days'].astype(float).round(1)
        print(summary_overall.tail(6))
    else:
        print("   (æ— æ•°æ®)")

    # 2. æ¯æœˆåˆ†ç±»ç»“æ¡ˆç»Ÿè®¡ (DataFrame é£æ ¼)
    monthly_status_agg = df_resolved.groupby([pd.Grouper(key='outcome_date', freq='ME'), 'case_status'])['reply_to_outcome_days'].agg(['count', 'mean'])
    
    # æ„å»ºå®½è¡¨ä¾›æ‰“å°
    status_summary = pd.DataFrame(index=pd.date_range(start=summary_overall.index[0] if not summary_overall.empty else '2025-01-01', 
                                                     periods=len(summary_overall), freq='ME'))
    status_summary.index = status_summary.index.strftime('%Y-%m')
    
    found_any = False
    for status in ['Granted', 'Dismissed']:
        if status in monthly_status_agg.index.get_level_values('case_status'):
            s_data = monthly_status_agg.xs(status, level='case_status')
            # è½¬æ¢ s_data ç´¢å¼•ä¸ºå­—ç¬¦ä¸²åŒ¹é…
            s_data.index = s_data.index.strftime('%Y-%m')
            
            status_summary[f'{status}_cnt'] = s_data['count']
            status_summary[f'{status}_avg(days)'] = s_data['mean']
            found_any = True
    
    print("\n--- æ¯æœˆåˆ†ç±»ç»“æ¡ˆç»Ÿè®¡ (Granted/Dismissed) (æœ€è¿‘ 6 ä¸ªæœˆ) ---")
    if found_any:
        # åªä¿ç•™æœ‰æ•°æ®çš„åˆ—
        cols = [c for c in status_summary.columns if status_summary[c].notna().any()]
        if cols:
            # ç¡®ä¿æ•°å€¼æ ¼å¼ä¸€è‡´ï¼Œå››èˆäº”å…¥åˆ°1ä½å°æ•°
            for col in cols:
                if '_cnt' in col:
                    status_summary[col] = status_summary[col].fillna(0).astype(int)
                else:
                    status_summary[col] = status_summary[col].fillna(0).astype(float).round(1)
            print(status_summary[cols].tail(6))
        else:
            print("   (æ— æ•°æ®)")
    else:
        print("   (æœªå‘ç° Granted æˆ– Dismissed æ¡ˆä¾‹æ•°æ®)")

    # 3. æ€»è®¡
    total_count = len(df_resolved)
    avg_duration = df_resolved['reply_to_outcome_days'].mean()
    print(f"\nã€æ€»ä½“æ±‡æ€»ã€‘ ç»“æ¡ˆæ€»æ•°: {total_count} | æ€»ä½“å¹³å‡è€—æ—¶: {avg_duration:.1f} å¤©")
    print("=" * 60)


def plot_case_duration_distribution(df):
    """ç»˜åˆ¶ç»“æ¡ˆè€—æ—¶åˆ†å¸ƒç›´æ–¹å›¾"""
    # ä»…ç»Ÿè®¡å·²ç»“æ¡ˆä¸”æœ‰è€—æ—¶æ•°æ®çš„ Mandamus æ¡ˆä»¶
    df_resolved = df[df['case_status'].isin(['Discontinued', 'Granted', 'Dismissed'])].copy()
    if df_resolved.empty or 'time_to_close' not in df_resolved.columns or df_resolved['time_to_close'].isna().all():
        return

    durations = df_resolved['time_to_close'].dropna()
    
    plt.figure(figsize=(12, 6))
    
    # è‡ªåŠ¨ç¡®å®š bin æ•°é‡
    sns.histplot(durations, bins=min(30, len(durations.unique())), kde=True, color='teal', alpha=0.6)
    
    # æ·»åŠ ä¸­ä½æ•°çº¿
    median_val = durations.median()
    plt.axvline(median_val, color='red', linestyle='--', linewidth=2, label=f'ä¸­ä½æ•°: {median_val:.1f}å¤©')
    
    title = 'Mandamus æ¡ˆä»¶ç»“æ¡ˆæ—¶é•¿åˆ†å¸ƒ (Filing to Outcome)'
    xlabel = 'ç»“æ¡ˆè€—æ—¶ (å¤©)'
    ylabel = 'æ¡ˆä»¶æ•°é‡'
    
    if _cjk_prop:
        plt.title(title, fontproperties=_cjk_prop, fontsize=16)
        plt.xlabel(xlabel, fontproperties=_cjk_prop)
        plt.ylabel(ylabel, fontproperties=_cjk_prop)
        plt.legend(prop=_cjk_prop)
    else:
        plt.title(title, fontsize=16)
        plt.xlabel(xlabel)
        plt.ylabel(ylabel)
        plt.legend()
        
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    save_path = os.path.join(OUTPUT_DIR, 'mandamus_duration_distribution.png')
    plt.savefig(save_path)
    print(f"ğŸ“ˆ å·²ä¿å­˜ç»“æ¡ˆè€—æ—¶åˆ†å¸ƒå›¾è‡³: {save_path}")
    plt.close()


def analyze_resolution_time_distribution(df):
    """ç»Ÿè®¡ä¸åŒç»“æ¡ˆæ—¶é•¿ (age_of_case at resolution) çš„æ¡ˆä»¶åˆ†å¸ƒ"""
    # ç­›é€‰å·²ç»“æ¡ˆçš„æ¡ˆä»¶
    df_resolved = df[df['case_status'].isin(['Discontinued', 'Granted', 'Dismissed'])].copy()
    
    if df_resolved.empty or 'time_to_close' not in df_resolved.columns or df_resolved['time_to_close'].isna().all():
        print("\n--- ç»“æ¡ˆè€—æ—¶åˆ†å¸ƒç»Ÿè®¡ ---")
        print("   (æ²¡æœ‰æœ‰æ•ˆçš„ç»“æ¡ˆè€—æ—¶æ•°æ®)")
        return

    # time_to_close å³ä¸ºç»“æ¡ˆæ—¶çš„ age_of_case
    durations = df_resolved['time_to_close'].dropna()
    
    print("\n--- ç»“æ¡ˆè€—æ—¶åˆ†å¸ƒç»Ÿè®¡ (Mandamus) ---")
    
    # å®šä¹‰åŒºé—´ (0, 30, 60, ..., 240, 365, +inf)
    bins = [0, 30, 60, 90, 120, 150, 180, 210, 240, 365, float('inf')]
    labels = ['0-30å¤©', '31-60å¤©', '61-90å¤©', '91-120å¤©', '121-150å¤©', '151-180å¤©', '181-210å¤©', '211-240å¤©', '241-365å¤©', '365å¤©ä»¥ä¸Š']
    
    # ç»Ÿè®¡
    dist = pd.cut(durations, bins=bins, labels=labels, right=True).value_counts().sort_index()
    total = len(durations)
    
    for label, count in dist.items():
        percentage = (count / total) * 100 if total > 0 else 0
        print(f"   {label:12}: {count:>4} æ¡ˆ ({percentage:>4.1f}%)")
    
    # æ‰¾å‡ºå¤šæ•°æ¡ˆä»¶æ‰€åœ¨èŒƒå›´
    if total > 0:
        most_common_range = dist.idxmax()
        print(f"\nğŸ“Š ç»“è®º: å¤šæ•° Mandamus æ¡ˆä»¶åœ¨ {most_common_range} èŒƒå›´å†…ç»“æ¡ˆã€‚")
    print("-" * 50)


def run_monthly_analysis(df):
    """
    å®ç°æŒ‰æœˆç»Ÿè®¡çš„é€»è¾‘ï¼Œå¥å£®å¤„ç†æ²¡æœ‰æ—¥æœŸæˆ–å…¨éƒ¨ä¸º NaT çš„æƒ…å†µã€‚
    """

    df = df.copy()
    df['filing_date'] = pd.to_datetime(df.get('filing_date'), errors='coerce')
    df['outcome_date'] = pd.to_datetime(df.get('outcome_date'), errors='coerce')

    # å¦‚æœæ—¢æ²¡æœ‰ filing_date ä¹Ÿæ²¡æœ‰ outcome_dateï¼Œåˆ™æ— æ³•åšæŒ‰æœˆåˆ†æ
    if not (df['filing_date'].notna().any() or df['outcome_date'].notna().any()):
        print("æ— æœ‰æ•ˆæ—¥æœŸæ•°æ®ï¼Œæ— æ³•è¿›è¡ŒæŒ‰æœˆåˆ†æã€‚")
        return

    # æŒ‰ filing_date (æ³¨å†Œæ—¥æœŸ) ç»Ÿè®¡æ¯æœˆæ³¨å†Œé‡
    if df['filing_date'].notna().any():
        df_filed_monthly = df.groupby(pd.Grouper(key='filing_date', freq='ME'))['case_number'].count().rename('filing_count')
    else:
        df_filed_monthly = pd.Series(dtype='int64', name='filing_count')

    # æŒ‰ outcome_date (ç»“æ¡ˆæ—¥æœŸ) ç»Ÿè®¡æ¯æœˆç»“æ¡ˆé‡
    df_resolved = df[df['case_status'].isin(['Discontinued', 'Granted', 'Dismissed'])]
    if (not df_resolved.empty) and df_resolved['outcome_date'].notna().any():
        df_resolved_monthly = df_resolved.groupby(pd.Grouper(key='outcome_date', freq='ME'))['case_number'].count().rename('resolution_count')
    else:
        df_resolved_monthly = pd.Series(dtype='int64', name='resolution_count')

    # åˆå¹¶æ•°æ®å¹¶è®¡ç®—å‡€ç§¯å‹å˜åŒ–
    df_monthly = pd.concat([df_filed_monthly, df_resolved_monthly], axis=1).fillna(0)
    df_monthly['net_change'] = df_monthly['filing_count'] - df_monthly['resolution_count']

    # ç»“æ¡ˆæ–¹å¼è¶‹åŠ¿
    def _safe_group(res_df, status, col='outcome_date'):
        if res_df.empty or res_df[col].notna().sum() == 0:
            return pd.Series(dtype='int64')
        return res_df[res_df['case_status'] == status].groupby(pd.Grouper(key=col, freq='ME'))['case_number'].count().rename(f"{status.lower()}_count")

    df_monthly['settled_count'] = _safe_group(df_resolved, 'Discontinued')
    df_monthly['granted_count'] = _safe_group(df_resolved, 'Granted')
    df_monthly['dismissed_count'] = _safe_group(df_resolved, 'Dismissed')

    # å…³é”®è€—æ—¶è¶‹åŠ¿ (ä¸­ä½æ•°)
    if (not df_resolved.empty) and df_resolved['outcome_date'].notna().any() and df_resolved['time_to_close'].notna().any():
        df_time_to_close_monthly = df_resolved.groupby(pd.Grouper(key='outcome_date', freq='ME'))['time_to_close'].median().rename('median_time_to_close')
        df_monthly = pd.concat([df_monthly, df_time_to_close_monthly], axis=1)
    else:
        df_monthly['median_time_to_close'] = np.nan
    
    # Memo å“åº”æ—¶é—´è¶‹åŠ¿
    df_with_memo = df[df['memo_response_time'].notna()]
    if not df_with_memo.empty:
        df_memo_monthly = df_with_memo.groupby(pd.Grouper(key='filing_date', freq='ME'))['memo_response_time'].median().rename('median_memo_response_time')
        df_monthly = pd.concat([df_monthly, df_memo_monthly], axis=1)
    else:
        df_monthly['median_memo_response_time'] = np.nan

    if df_monthly.empty:
        print("æ²¡æœ‰ç”Ÿæˆä»»ä½•æŒ‰æœˆç»Ÿè®¡æ•°æ®ã€‚")
        return

    # ç»˜åˆ¶å›¾è¡¨
    plot_workload_trends(df_monthly)
    plot_outcome_trends(df_monthly)
    plot_timeline_trends(df_monthly)
    plot_memo_response_trends(df_monthly)
    # æ–°å¢ï¼šæŒ‰æœˆç»Ÿè®¡ memo å›å¤ åˆ° ç»“æ¡ˆ çš„æ—¶é—´ï¼ŒæŒ‰ç»“æ¡ˆç±»å‹åˆ†ç³»åˆ—
    try:
        plot_memo_reply_to_outcome_trends(df)
    except Exception as e:
        print('ç»˜åˆ¶ memo->outcome è¶‹åŠ¿å¤±è´¥ï¼š', e)

    # æ–°å¢ï¼šç»“æ¡ˆè€—æ—¶åˆ†å¸ƒåˆ†æ
    try:
        plot_case_duration_distribution(df)
    except Exception as e:
        print('ç»˜åˆ¶ç»“æ¡ˆæ—¶é•¿åˆ†å¸ƒå›¾å¤±è´¥ï¼š', e)

    # æ‰“å°æ–‡å­—æŠ¥å‘Š
    print("\n" + "="*50)
    print("ã€2025 å¹´æŒ‰æœˆç»Ÿè®¡è¶‹åŠ¿åˆ†ææŠ¥å‘Šã€‘")
    print("="*50)
    print("\n--- æ¡ˆä»¶è´Ÿè·ä¸ç§¯å‹å˜åŒ– (æœ€è¿‘ 6 ä¸ªæœˆ) ---")
    print(df_monthly[['filing_count', 'resolution_count', 'net_change']].tail(6).round(0).astype(int))

    # æ–°å¢æ–‡å­—ç‰ˆåˆ†å¸ƒç»Ÿè®¡
    analyze_resolution_time_distribution(df)

    print("\n--- ç»“æ¡ˆæ–¹å¼ç™¾åˆ†æ¯” (æœ€è¿‘ 6 ä¸ªæœˆ) ---")
    df_recent_outcome = df_monthly.tail(6).copy()
    df_recent_outcome['resolution_total'] = df_recent_outcome[['settled_count', 'granted_count', 'dismissed_count']].sum(axis=1)
    # avoid division by zero
    df_recent_outcome['Settled Rate'] = df_recent_outcome.apply(lambda r: (f"{round(r['settled_count']/r['resolution_total']*100, 1)}%") if r['resolution_total']>0 else '0.0%', axis=1)
    df_recent_outcome['Granted Rate'] = df_recent_outcome.apply(lambda r: (f"{round(r['granted_count']/r['resolution_total']*100, 1)}%") if r['resolution_total']>0 else '0.0%', axis=1)
    df_recent_outcome['Dismiss Rate'] = df_recent_outcome.apply(lambda r: (f"{round(r['dismissed_count']/r['resolution_total']*100, 1)}%") if r['resolution_total']>0 else '0.0%', axis=1)
    # å¯¹ä¸­ä½æ•°è€—æ—¶è¿›è¡Œå››èˆäº”å…¥
    df_recent_outcome['median_time_to_close'] = df_recent_outcome['median_time_to_close'].round(1)
    print(df_recent_outcome[['resolution_total', 'Settled Rate', 'Granted Rate', 'Dismiss Rate', 'median_time_to_close']])
    
    # Memo å“åº”æ—¶é—´ç»Ÿè®¡
    df_with_memo = df[df['memo_response_time'].notna()]
    if not df_with_memo.empty:
        print("\n--- DOJ Memo å“åº”æ—¶é—´ç»Ÿè®¡ ---")
        overall_avg = df_with_memo['memo_response_time'].mean()
        overall_median = df_with_memo['memo_response_time'].median()
        
        print(f"æ€»ä½“ Memo å“åº”æ—¶é—´:")
        print(f"   å¹³å‡å€¼: {overall_avg:.1f} å¤©")
        print(f"   ä¸­ä½æ•°: {overall_median:.1f} å¤©")
        print(f"   æœ€å¿«: {df_with_memo['memo_response_time'].min()} å¤©")
        print(f"   æœ€æ…¢: {df_with_memo['memo_response_time'].max()} å¤©")
        print(f"   æ•°æ®è¦†ç›–ç‡: {len(df_with_memo)/len(df)*100:.1f}%")
        
        # æŒ‰çŠ¶æ€åˆ†æ
        status_memo_stats = df_with_memo.groupby('case_status')['memo_response_time'].agg(['count', 'mean', 'median'])
        print(f"\næŒ‰æ¡ˆä»¶çŠ¶æ€åˆ†æ:")
        for status, stats in status_memo_stats.iterrows():
            print(f"   {status[0]}: {stats['count']} æ¡ˆ, å¹³å‡ {stats['mean']:.1f} å¤©, ä¸­ä½æ•° {stats['median']:.1f} å¤©")
        
        if 'median_memo_response_time' in df_monthly.columns:
            print("\n--- æœ€è¿‘ 6 ä¸ªæœˆ Memo å“åº”æ—¶é—´è¶‹åŠ¿ ---")
            recent_memo = df_monthly[['median_memo_response_time']].tail(6).round(1)
            print(recent_memo)
    else:
        print("\n--- DOJ Memo å“åº”æ—¶é—´ç»Ÿè®¡ ---")
        print("   æœªæ‰¾åˆ° Memo å“åº”æ—¶é—´æ•°æ®")

    print("\nã€è¶‹åŠ¿è§£è¯»ã€‘")
    if df_monthly['net_change'].tail(3).mean() > 0:
        print("-> ğŸš¨ è­¦å‘Šï¼šè¿‘ä¸‰ä¸ªæœˆå‡€ç§¯å‹å˜åŒ–å¹³å‡ä¸ºæ­£å€¼ï¼Œæ³•é™¢/IRCC æ­£åœ¨æ‰¿å—æ›´å¤§å‹åŠ›ï¼Œæœªæ¥æ¡ˆä»¶å¤„ç†é€Ÿåº¦å¯èƒ½ä¼šå‡æ…¢ã€‚")
    elif df_monthly['median_time_to_close'].tail(3).mean() > df_monthly['median_time_to_close'].iloc[:-3].mean():
        print("-> âš ï¸ æ³¨æ„ï¼šå°½ç®¡ç§¯å‹å˜åŒ–ä¸æ˜æ˜¾ï¼Œä½†ç»“æ¡ˆæ‰€éœ€çš„ä¸­ä½æ•°æ—¶é—´ä»åœ¨å¢åŠ ï¼Œè¡¨æ˜æ•ˆç‡æœ‰æ‰€ä¸‹é™ã€‚")
    else:
        print("-> âœ… ç¨³å®šï¼šç›®å‰æ¡ˆä»¶ç§¯å‹è¶‹åŠ¿å’Œç»“æ¡ˆè€—æ—¶è¾ƒä¸ºç¨³å®šã€‚")



# --- ä¸»æ‰§è¡ŒåŒº ---
def main():
    parser = argparse.ArgumentParser(description='FCT Mandamus æ¡ˆä»¶åˆ†æä¸æ•°æ®å¯¼å‡º')
    parser.add_argument('--year', type=int, default=2025, help='è¦åˆ†æå’Œå¯¼å‡ºçš„å¹´ä»½ (é»˜è®¤: 2025)')
    args = parser.parse_args()
    
    target_year = args.year
    
    # 1. æå–æ ¸å¿ƒæ•°æ®
    df_core = get_mandamus_data_for_analysis(target_year)
    
    # 2. è¿è¡ŒæŒ‰æœˆåˆ†æå¹¶ç»˜åˆ¶å›¾è¡¨
    if not df_core.empty:
        run_monthly_analysis(df_core)
        
        # 3. é¢å¤–åŠŸèƒ½ï¼šå¯¼å‡ºè¯¦ç»†ä¿¡æ¯ä¸º JSON
        export_cases_to_json(target_year)
    else:
        print(f"æœªæ‰¾åˆ° {target_year} å¹´ Mandamus æ¡ˆä»¶æ•°æ®è¿›è¡Œåˆ†æã€‚")

    # æ³¨æ„ï¼šå¾®è§‚åˆ†æ (Memo to Outcome) éœ€è¦ docket_entries è¡¨ï¼Œè¯·åœ¨å®é™…è¿è¡Œä¸­æ•´åˆ V3 å’Œ V4 è„šæœ¬ã€‚

####################################################33
if __name__ == "__main__":
    main()